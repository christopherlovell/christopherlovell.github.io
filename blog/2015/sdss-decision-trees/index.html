<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Classifying Astronomical Data Using Tree Based Methods | Chris Lovell </title> <meta name="author" content="Chris Lovell"> <meta name="description" content="Computational Astrophysicist "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A6%86&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://christopherlovell.co.uk/blog/2015/sdss-decision-trees/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Chris</span> Lovell </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Classifying Astronomical Data Using Tree Based Methods</h1> <p class="post-meta"> Created in November 14, 2015 </p> <p class="post-tags"> <a href="/blog/2015"> <i class="fa-solid fa-calendar fa-sm"></i> 2015 </a>   ·   <a href="/blog/tag/data-science"> <i class="fa-solid fa-hashtag fa-sm"></i> Data Science</a>   <a href="/blog/tag/physics"> <i class="fa-solid fa-hashtag fa-sm"></i> Physics</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>The following is a guide to using tree based methods in R, based on the corresponding chapter in ‘An Introduction to Statistical Learning’ but using data from the <strong>Sloan Digital Sky Survey</strong> (SDSS). The aim is to use the five colour bands provided by the SDSS extract, <em>u</em> (ultraviolet), <em>g</em> (green), <em>r</em> (red), <em>i</em> &amp; <em>z</em> (very-near-infrared), to predict whether the sources in the survey are Quasars, Stars or White Dwarfs. I use a variety of techniques, from simple decision trees to ensemble methods such as random forests.</p> <h2 id="data">Data</h2> <p>Rather than getting the data directly from SDSS and doing the cleaning myself, I’m going to cheat and use a pre-filtered data set used in the book ‘Modern Statistical Methods for Astronomy’, available <a href="http://astrostatistics.psu.edu/MSMA/datasets/index.html" rel="external nofollow noopener" target="_blank">here</a>. As part of their extract they perform a few cleaning operations, such as ignoring spatially resolved galaxies, those with large measurement errors, and those that are very bright (and could cause saturation) or very faint (with uncertain measurements). They also provide 3 labelled data sets for training, one each for Quasars, Stars &amp; White Dwarfs.</p> <p>The colour bands as they stand aren’t particularly useful, since objects of the same class can be at different distances, and therefore have relatively lower flux across all bands. This can be avoided by looking at the ratios of brightness across bands, and since magnitudes are logarithmic units of brightness we simply find the difference between the provided values to get four colour indices, <em>(u-g)</em>, <em>(g-r)</em>, <em>(r-i)</em> &amp; <em>(i-z)</em>.</p> <p>The following three code chunks extract and clean the training data for all three sources and combine them in to a single data frame. Quasras, stars and white dwarfs are given the labels 1,2 and 3 respectively. There are 5000 stellar objects available for training, but for quasars there are over 7.7429 × 10<sup>4</sup> and for white dwarfs over 1.009 × 10<sup>4</sup>, so I’ve filtered each of the latter two down to only 5000 so that there are equal numbers of each class.</p> <p>Quasar training set (Class 1):</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">dat1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.table</span><span class="p">(</span><span class="s1">'http://astrostatistics.psu.edu/MSMA/datasets/SDSS_QSO.dat'</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="o">=</span><span class="nb">T</span><span class="p">)</span><span class="w">  
</span><span class="n">bad_phot_qso</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">which</span><span class="p">(</span><span class="n">dat1</span><span class="p">[,</span><span class="nf">c</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="m">5</span><span class="p">,</span><span class="m">7</span><span class="p">,</span><span class="m">9</span><span class="p">,</span><span class="m">11</span><span class="p">)]</span><span class="w"> </span><span class="o">&gt;</span><span class="w"> </span><span class="m">21.0</span><span class="w"> </span><span class="o">|</span><span class="w"> </span><span class="n">dat1</span><span class="p">[,</span><span class="m">3</span><span class="p">]</span><span class="o">==</span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="n">dat1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dat1</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="o">-</span><span class="n">bad_phot_qso</span><span class="p">,]</span><span class="w">
</span><span class="n">dat1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">((</span><span class="n">dat1</span><span class="p">[,</span><span class="m">3</span><span class="p">]</span><span class="o">-</span><span class="n">dat1</span><span class="p">[,</span><span class="m">5</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">dat1</span><span class="p">[,</span><span class="m">5</span><span class="p">]</span><span class="o">-</span><span class="n">dat1</span><span class="p">[,</span><span class="m">7</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">dat1</span><span class="p">[,</span><span class="m">7</span><span class="p">]</span><span class="o">-</span><span class="n">dat1</span><span class="p">[,</span><span class="m">9</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">dat1</span><span class="p">[,</span><span class="m">9</span><span class="p">]</span><span class="o">-</span><span class="n">dat1</span><span class="p">[,</span><span class="m">11</span><span class="p">]))</span><span class="w">
</span><span class="n">qso_train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">dat1</span><span class="p">,</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">dat1</span><span class="p">[,</span><span class="m">1</span><span class="p">]))))</span><span class="w">
</span><span class="nf">names</span><span class="p">(</span><span class="n">qso_train</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'u_g'</span><span class="p">,</span><span class="w"> </span><span class="s1">'g_r'</span><span class="p">,</span><span class="w"> </span><span class="s1">'r_i'</span><span class="p">,</span><span class="w"> </span><span class="s1">'i_z'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Class'</span><span class="p">)</span></code></pre></figure> <p>Star training set (Class 2):</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">dat2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s1">'http://astrostatistics.psu.edu/MSMA/datasets/SDSS_stars.csv'</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="o">=</span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">dat2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">((</span><span class="n">dat2</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="o">-</span><span class="n">dat2</span><span class="p">[,</span><span class="m">2</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">dat2</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="o">-</span><span class="n">dat2</span><span class="p">[,</span><span class="m">3</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">dat2</span><span class="p">[,</span><span class="m">3</span><span class="p">]</span><span class="o">-</span><span class="n">dat2</span><span class="p">[,</span><span class="m">4</span><span class="p">]),</span><span class="w">
	</span><span class="p">(</span><span class="n">dat2</span><span class="p">[,</span><span class="m">4</span><span class="p">]</span><span class="o">-</span><span class="n">dat2</span><span class="p">[,</span><span class="m">5</span><span class="p">]))</span><span class="w">
</span><span class="n">star_train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">dat2</span><span class="p">,</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">dat2</span><span class="p">[,</span><span class="m">1</span><span class="p">]))))</span><span class="w">
</span><span class="nf">names</span><span class="p">(</span><span class="n">star_train</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'u_g'</span><span class="p">,</span><span class="s1">'g_r'</span><span class="p">,</span><span class="s1">'r_i'</span><span class="p">,</span><span class="s1">'i_z'</span><span class="p">,</span><span class="s1">'Class'</span><span class="p">)</span></code></pre></figure> <p>White dwarf training set (Class 3):</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">dat3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s1">'http://astrostatistics.psu.edu/MSMA/datasets/SDSS_wd.csv'</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="o">=</span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">dat3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">na.omit</span><span class="p">(</span><span class="n">dat3</span><span class="p">)</span><span class="w">
</span><span class="n">dat3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">((</span><span class="n">dat3</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="m">2</span><span class="p">]</span><span class="o">-</span><span class="n">dat3</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="m">3</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">dat3</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="m">3</span><span class="p">]</span><span class="o">-</span><span class="n">dat3</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="m">4</span><span class="p">]),(</span><span class="n">dat3</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="m">4</span><span class="p">]</span><span class="o">-</span><span class="n">dat3</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="m">5</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">dat3</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="m">5</span><span class="p">]</span><span class="o">-</span><span class="n">dat3</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">5000</span><span class="p">,</span><span class="m">6</span><span class="p">]))</span><span class="w">

</span><span class="n">wd_train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">cbind</span><span class="p">(</span><span class="n">dat3</span><span class="p">,</span><span class="w"> </span><span class="nf">rep</span><span class="p">(</span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="nf">length</span><span class="p">(</span><span class="n">dat3</span><span class="p">[,</span><span class="m">1</span><span class="p">]))))</span><span class="w">
</span><span class="nf">names</span><span class="p">(</span><span class="n">wd_train</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'u_g'</span><span class="p">,</span><span class="w"> </span><span class="s1">'g_r'</span><span class="p">,</span><span class="w"> </span><span class="s1">'r_i'</span><span class="p">,</span><span class="w"> </span><span class="s1">'i_z'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Class'</span><span class="p">)</span></code></pre></figure> <p>Combine the training sets</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">SDSS_train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">rbind</span><span class="p">(</span><span class="n">qso_train</span><span class="p">,</span><span class="w"> </span><span class="n">star_train</span><span class="p">,</span><span class="w"> </span><span class="n">wd_train</span><span class="p">))</span><span class="w">
</span><span class="nf">names</span><span class="p">(</span><span class="n">SDSS_train</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'u_g'</span><span class="p">,</span><span class="w"> </span><span class="s1">'g_r'</span><span class="p">,</span><span class="w"> </span><span class="s1">'r_i'</span><span class="p">,</span><span class="w"> </span><span class="s1">'i_z'</span><span class="p">,</span><span class="w"> </span><span class="s1">'Class'</span><span class="p">)</span><span class="w">
</span><span class="n">str</span><span class="p">(</span><span class="n">SDSS_train</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-text" data-lang="text">## 'data.frame':	15000 obs. of  5 variables:
##  $ u_g  : num  -0.079 0.033 0.11 0.325 0.22 ...
##  $ g_r  : num  0.136 0.255 0.425 0.448 0.049 ...
##  $ r_i  : num  0.233 0.454 0.221 0.114 0.189 ...
##  $ i_z  : num  0.046 0.3 -0.158 0.221 0.04 ...
##  $ Class: num  1 1 1 1 1 1 1 1 1 1 ...</code></pre></figure> <p>The plot below shows each training class on a bivariate colour-colour scatter plot.There’s plenty of structure to each class, something that tree based methods should be more than capable of picking up on.</p> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-5-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <h2 id="decision-trees">Decision Trees</h2> <p>Decision trees are the most basic tree based method, and one on which the majority of other methods are built on They work by splitting the predictor space in to regions; each split can be thought of as a <em>branch</em>, and each of the remaining regions are <em>leaves</em>.</p> <p>The default <code class="language-plaintext highlighter-rouge">tree</code> library has a simple binary recursive partitioning method for growing regression or classification trees.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span></code></pre></figure> <p>Below we split the data in to a training and test set, and train the classifier on the training test.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">SDSS_train</span><span class="o">$</span><span class="n">Class</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.factor</span><span class="p">(</span><span class="n">SDSS_train</span><span class="o">$</span><span class="n">Class</span><span class="p">)</span><span class="w">

</span><span class="n">set.seed</span><span class="p">(</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">train</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">sample</span><span class="p">(</span><span class="n">nrow</span><span class="p">(</span><span class="n">SDSS_train</span><span class="p">),</span><span class="w"> </span><span class="m">4</span><span class="o">*</span><span class="n">nrow</span><span class="p">(</span><span class="n">SDSS_train</span><span class="p">)</span><span class="o">/</span><span class="m">5</span><span class="p">)</span><span class="w">

</span><span class="n">tree.sdss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">tree</span><span class="p">(</span><span class="n">Class</span><span class="o">~</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">)</span></code></pre></figure> <p><code class="language-plaintext highlighter-rouge">tree.sdss</code> is our trained classifier. We can plot it to see the major branches and leaves of the tree. The default is pretty cluttered, so I’ve coloured and rotated the text to make it easier to read… not sure if that really helps. The <code class="language-plaintext highlighter-rouge">rpart</code> package for building trees has some nicer plotting capabilities but, in the spirit of every undergraduate lab report, ‘<em>that is beyond the purposes of this investigation</em>’.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">plot</span><span class="p">(</span><span class="n">tree.sdss</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">tree.sdss</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="n">rainbow</span><span class="p">(</span><span class="m">10</span><span class="p">)[</span><span class="m">1</span><span class="o">:</span><span class="m">25</span><span class="p">],</span><span class="n">srt</span><span class="o">=</span><span class="m">35</span><span class="p">,</span><span class="n">cex</span><span class="o">=</span><span class="m">0.8</span><span class="p">)</span></code></pre></figure> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-8-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <p>To evaluate our tree, we use it to predict the class of our test data.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">tree.pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">tree.sdss</span><span class="p">,</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,],</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"class"</span><span class="p">)</span></code></pre></figure> <p>Below is a <a href="https://en.wikipedia.org/wiki/Confusion_matrix" rel="external nofollow noopener" target="_blank">confusion matrix</a> of the predicted classes against the actual.</p> <figure class="highlight"><pre><code class="language-text" data-lang="text">##
## tree.pred   1   2   3
##         1 921   8 131
##         2  65 985   1
##         3  14   0 875</code></pre></figure> <p>The overall test error rate is 7.3%.</p> <p>Often the algorithm that builds the tree can create more branches than necessary, and end up reducing the predictive accuracy of our classifier. To test this I perform cross validation on the built tree. Specifying <code class="language-plaintext highlighter-rouge">FUN = prune.misclass</code> tells <code class="language-plaintext highlighter-rouge">cv.tree</code> that we want the cross validation to be guided by the classification error rate, rather than the default which is the <a href="https://en.wikipedia.org/wiki/Deviance_(statistics)" rel="external nofollow noopener" target="_blank">deviance</a>.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">cv.sdss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cv.tree</span><span class="p">(</span><span class="n">tree.sdss</span><span class="p">,</span><span class="w"> </span><span class="n">FUN</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">prune.misclass</span><span class="p">)</span><span class="w">
</span><span class="n">cv.sdss</span></code></pre></figure> <figure class="highlight"><pre><code class="language-text" data-lang="text">## $size
## [1] 11 10  9  7  6  5  3  2  1
##
## $dev
## [1]  868  868  922  971 1039 1194 1592 4050 8177
##
## $k
## [1]   -Inf    0.0   38.0   44.5   72.0  111.0  217.0 2482.0 3943.0
##
## $method
## [1] "misclass"
##
## attr(,"class")
## [1] "prune"         "tree.sequence"</code></pre></figure> <p>The left hand plot shows the error rate against tree size, the right against the cost complexity parameter <code class="language-plaintext highlighter-rouge">k</code>.</p> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-12-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <p>The 10 branch tree has the same error rate as the 11, so pruning to this size will not reduce the predictive power of the model, but will reduce the complexity.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">min_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">cv.sdss</span><span class="o">$</span><span class="n">size</span><span class="p">[</span><span class="nf">max</span><span class="p">(</span><span class="n">which</span><span class="p">(</span><span class="n">cv.sdss</span><span class="o">$</span><span class="n">dev</span><span class="w"> </span><span class="o">==</span><span class="w"> </span><span class="nf">min</span><span class="p">(</span><span class="n">cv.sdss</span><span class="o">$</span><span class="n">dev</span><span class="p">)))]</span><span class="w">  </span><span class="c1"># find minimum size that fits best</span><span class="w">

</span><span class="n">prune.sdss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">prune.misclass</span><span class="p">(</span><span class="n">tree.sdss</span><span class="p">,</span><span class="w"> </span><span class="n">best</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">min_size</span><span class="p">)</span><span class="w">
</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="m">1</span><span class="p">))</span><span class="w">
</span><span class="n">plot</span><span class="p">(</span><span class="n">prune.sdss</span><span class="p">)</span><span class="w">
</span><span class="n">text</span><span class="p">(</span><span class="n">tree.sdss</span><span class="p">,</span><span class="n">col</span><span class="o">=</span><span class="n">rainbow</span><span class="p">(</span><span class="m">10</span><span class="p">)[</span><span class="m">1</span><span class="o">:</span><span class="m">25</span><span class="p">],</span><span class="n">srt</span><span class="o">=</span><span class="m">35</span><span class="p">,</span><span class="n">cex</span><span class="o">=</span><span class="m">0.8</span><span class="p">)</span></code></pre></figure> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-14-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">tree.pred</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">prune.sdss</span><span class="p">,</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,],</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"class"</span><span class="p">)</span><span class="w">

</span><span class="n">test.results</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">table</span><span class="p">(</span><span class="n">tree.pred</span><span class="p">,</span><span class="n">SDSS_train</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,</span><span class="s2">"Class"</span><span class="p">])</span><span class="w">
</span><span class="n">test.results</span></code></pre></figure> <figure class="highlight"><pre><code class="language-text" data-lang="text">##
## tree.pred   1   2   3
##         1 921   8 131
##         2  65 985   1
##         3  14   0 875</code></pre></figure> <p>The error rate, 7.3%, is the same, as expected, but the tree is easier to interpret.</p> <h2 id="bagging-and-random-forests">Bagging and Random Forests</h2> <p>Bagging and random forests are both examples of ensemble methods, where many decison trees are combined together to improve the prediction accuracy. Both can be implemented using the <code class="language-plaintext highlighter-rouge">randomForest</code> package.</p> <p>Bagging (derived from the full name <em>Bootstrap Aggregation</em>) takes multiple bootstrapped samples from the same training set and builds an ensemble of trees that are then averaged. Bagging uses all predictors; <code class="language-plaintext highlighter-rouge">mtry</code> states that all 4 predictors should be considered for each split of the tree.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">randomForest</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">bag.sdss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">randomForest</span><span class="p">(</span><span class="n">Class</span><span class="o">~</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">mtry</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">importance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">bag.sdss</span></code></pre></figure> <figure class="highlight"><pre><code class="language-text" data-lang="text">##
## Call:
##  randomForest(formula = Class ~ ., data = SDSS_train, mtry = 4,      importance = T, subset = train)
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 4
##
##         OOB estimate of  error rate: 2.38%
## Confusion matrix:
##      1    2    3 class.error
## 1 3853   55   92 0.036750000
## 2   25 3981    1 0.006488645
## 3  111    1 3881 0.028049086</code></pre></figure> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">yhat.bag</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">bag.sdss</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,])</span></code></pre></figure> <figure class="highlight"><pre><code class="language-text" data-lang="text">##
## yhat.bag   1   2   3
##        1 966   5  36
##        2  12 988   0
##        3  22   0 971</code></pre></figure> <p>The test error rate associated with the bagged tree is 2.5%, a significant improvement over the single decision tree.</p> <p>Random forests are similar to bagged trees, but with a small tweak to the algorithm; at each step, when a split is considered only a <em>random subset</em> of the predictors is made available. This prevents strong features from dominating the root branches of the trees, otherwise this can lead to correlations between the predictions of the trees, as they all look relatively similar. The trees in a random forest ensemble can be thought of as <em>decorrelated</em>.</p> <p>Growing a random forest proceeds in the same way as Bagging, but with a smaller value for <code class="language-plaintext highlighter-rouge">mtry</code>. By default, for classification problems <code class="language-plaintext highlighter-rouge">randomForst</code> uses $\sqrt{p}$ predictors, so 2 in our case.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">rf.sdss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">randomForest</span><span class="p">(</span><span class="n">Class</span><span class="o">~</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">,</span><span class="w"> </span><span class="n">subset</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">train</span><span class="p">,</span><span class="w"> </span><span class="n">importance</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">rf.sdss</span></code></pre></figure> <figure class="highlight"><pre><code class="language-text" data-lang="text">##
## Call:
##  randomForest(formula = Class ~ ., data = SDSS_train, importance = T,      subset = train)
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 2
##
##         OOB estimate of  error rate: 2.17%
## Confusion matrix:
##      1    2    3 class.error
## 1 3869   50   81 0.032750000
## 2   23 3983    1 0.005989518
## 3  104    2 3887 0.026546456</code></pre></figure> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">yhat.rf</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">rf.sdss</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,])</span><span class="w">
</span><span class="n">test.results</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">table</span><span class="p">(</span><span class="n">yhat.rf</span><span class="p">,</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,</span><span class="s2">"Class"</span><span class="p">])</span></code></pre></figure> <p>The test error rate associated with the random forest is 2.37%, a further improvement over the bagged tree.</p> <p>We can use the <code class="language-plaintext highlighter-rouge">importance</code> function to view the importance of each of the variables used as our features. The first, <code class="language-plaintext highlighter-rouge">%IncMSE</code>, measures the mean decrease in accuracy of the predictions on out of bag samples when that feature is excluded from the model. The second, <code class="language-plaintext highlighter-rouge">IncNodePurity</code>, measures the decrease in node impurity due to splits over that variable, over all trees; node impurity measured by training RSS in the case of regression trees, and deviance for classification trees. <code class="language-plaintext highlighter-rouge">varImpPlot</code> plots these importance functions.</p> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-22-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <h2 id="boosting">Boosting</h2> <p>Boosting algortihms for regression and classification problems are different, and I will not provide a full description here (for details, see <a href="https://www.statsoft.com/Textbook/Boosting-Trees-Regression-Classification/button/1" rel="external nofollow noopener" target="_blank">here</a>). In basic terms, boosting algorithms apply many weak learners sequentially to the residuals (i.e. the remaining unexplained data) of previous trees. The algorithm learns slowly and incrementally, which can lead to a better resulting model, at the cost of extra computation compared to more direct learners.</p> <p>The <code class="language-plaintext highlighter-rouge">gbm</code> function, from the identically named package, is used here to perform Boosting.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">boost.sdss</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">gbm</span><span class="p">(</span><span class="n">Class</span><span class="o">~</span><span class="n">.</span><span class="p">,</span><span class="w"> </span><span class="n">data</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="n">train</span><span class="p">,],</span><span class="w"> </span><span class="n">distribution</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"multinomial"</span><span class="p">,</span><span class="w"> </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w"> </span><span class="n">interaction.depth</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">4</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">summary</span><span class="p">(</span><span class="n">boost.sdss</span><span class="p">)</span></code></pre></figure> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-26-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <figure class="highlight"><pre><code class="language-text" data-lang="text">##     var   rel.inf
## u_g u_g 46.908265
## g_r g_r 23.571429
## r_i r_i 22.557859
## i_z i_z  6.962446</code></pre></figure> <p>The plot above shows the relative importance of each feature in the training data. The <code class="language-plaintext highlighter-rouge">interaction.depth</code> argument, in the cal to <code class="language-plaintext highlighter-rouge">gbm</code>, limits the depth of each tree. Here we use a multinomial distribution as this is a multinomial classification problem; if it was binary, use a bernoulli distribution, or if performing a regression, use a gaussian distribution.</p> <p>Below are some <em>partial dependence plots</em>, which integrate out other variables to show the marginal effect of selected variables. The black line shows class 1, the red line class 2, and green class 3 (Quasars, Stars &amp; White Dwarfs respectively). The peaks of each line show where for this line ratio that particular class can be identified most clearly.</p> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-27-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">yhat.boost</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">boost.sdss</span><span class="p">,</span><span class="w"> </span><span class="n">newdata</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,],</span><span class="w"> </span><span class="n">n.trees</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">5000</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s1">'response'</span><span class="p">)</span><span class="w">
</span><span class="n">yhat.boost</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">apply</span><span class="p">(</span><span class="n">yhat.boost</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">which.max</span><span class="p">)</span><span class="w"> </span><span class="c1"># find max predictor</span></code></pre></figure> <figure class="highlight"><pre><code class="language-text" data-lang="text">##
## yhat.boost   1   2   3
##          1 953   3  47
##          2  28 990   0
##          3  19   0 960</code></pre></figure> <p>Test error rate associated with Boosting is 3.23%. This is actually <strong>worse</strong> than the bagging and random forest approaches above, for this particular data set, and the performance is $~\mathcal{O}(10)$ worse.</p> <h2 id="extremely-randomized-trees">Extremely randomized trees</h2> <p>Extremely Randomised Trees (ERTs) are a relatively modern incarnation of random forests. The difference is that, after choosing a random subset of features, the threshold for the split on each feature is also chosen randomly, and the best split is then chosen. Then ensemble of trees is again combined to provide the best estimate. This randomness increases the variance at the cost of a little bias.</p> <p>The <code class="language-plaintext highlighter-rouge">extraTrees</code> package in R can execute ERTs. Somme of the documentation looks a little rough around the edges, so I’d certainly take a closer look at the source code if you’re doing anything important with it. For our purposes though it will suffice.</p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">library</span><span class="p">(</span><span class="n">extraTrees</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">et</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">extraTrees</span><span class="p">(</span><span class="n">SDSS_train</span><span class="p">[</span><span class="n">train</span><span class="p">,</span><span class="m">-5</span><span class="p">],</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="n">train</span><span class="p">,</span><span class="s2">"Class"</span><span class="p">])</span><span class="w">
</span><span class="n">yhat.et</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">et</span><span class="p">,</span><span class="w"> </span><span class="n">SDSS_train</span><span class="p">[</span><span class="o">-</span><span class="n">train</span><span class="p">,</span><span class="m">-5</span><span class="p">])</span></code></pre></figure> <figure class="highlight"><pre><code class="language-text" data-lang="text">##
## yhat.et   1   2   3
##       1 972   3  35
##       2  12 990   0
##       3  16   0 972</code></pre></figure> <p>Test error rate associated with ERTs is 2.2%, the best of all the approaches demonstrated here.</p> <h2 id="sdss-test-data">SDSS ‘test’ data</h2> <p>The source of the data used in this post, the textbook ‘<a href="http://astrostatistics.psu.edu/MSMA/datasets/index.html" rel="external nofollow noopener" target="_blank">Modern Statistical Methods for Astronomy</a>’, made another set of SDSS data available named ‘test data’ that consist of 17000 sources. Unfortunately it doesn’t have any associated source classes, making it a pretty useless test set! However, it is useful to apply our models to and analyse from inspection. Here I use the random forest model, since it has one of the best error rate to complexity ratios. Below is a colour-colour plot similar to that made at the start of the workbook for the training data (repeated below for easier comparison).</p> <p><em>SDSS point sources test dataset, N=17,000 (mag&lt;21, point sources, hi-qual)</em></p> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">SDSS</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">read.csv</span><span class="p">(</span><span class="s1">'http://astrostatistics.psu.edu/MSMA/datasets/SDSS_test.csv'</span><span class="p">,</span><span class="w"> </span><span class="n">h</span><span class="o">=</span><span class="nb">T</span><span class="p">)</span><span class="w">
</span><span class="n">SDSS_test</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">data.frame</span><span class="p">(</span><span class="n">cbind</span><span class="p">((</span><span class="n">SDSS</span><span class="p">[,</span><span class="m">1</span><span class="p">]</span><span class="o">-</span><span class="n">SDSS</span><span class="p">[,</span><span class="m">2</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">SDSS</span><span class="p">[,</span><span class="m">2</span><span class="p">]</span><span class="o">-</span><span class="n">SDSS</span><span class="p">[,</span><span class="m">3</span><span class="p">]),</span><span class="w">
	</span><span class="p">(</span><span class="n">SDSS</span><span class="p">[,</span><span class="m">3</span><span class="p">]</span><span class="o">-</span><span class="n">SDSS</span><span class="p">[,</span><span class="m">4</span><span class="p">]),</span><span class="w"> </span><span class="p">(</span><span class="n">SDSS</span><span class="p">[,</span><span class="m">4</span><span class="p">]</span><span class="o">-</span><span class="n">SDSS</span><span class="p">[,</span><span class="m">5</span><span class="p">])))</span><span class="w">
</span><span class="nf">names</span><span class="p">(</span><span class="n">SDSS_test</span><span class="p">)</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="s1">'u_g'</span><span class="p">,</span><span class="w"> </span><span class="s1">'g_r'</span><span class="p">,</span><span class="w"> </span><span class="s1">'r_i'</span><span class="p">,</span><span class="w"> </span><span class="s1">'i_z'</span><span class="p">)</span></code></pre></figure> <figure class="highlight"><pre><code class="language-r" data-lang="r"><span class="n">SDSS_test</span><span class="o">$</span><span class="n">Class.Predict</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">predict</span><span class="p">(</span><span class="n">rf.sdss</span><span class="p">,</span><span class="w"> </span><span class="n">SDSS_test</span><span class="p">)</span><span class="w">
</span><span class="c1">#yhat.boost &lt;- predict(boost.sdss, SDSS_test, n.trees = 5000, type='response')</span><span class="w">
</span><span class="c1">#SDSS_test$Class.Predict &lt;- apply(yhat.boost, 1, which.max) # find max predictor</span></code></pre></figure> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-35-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <p><img src="/../images/SDSS_decision_trees/unnamed-chunk-36-1.png" title="center" alt="center" style="display: block; margin: auto;"></p> <p>There are a few interesting features here. Firstly, there are hardly any white dwarfs identified. This could be because there aren’t very many in this data set, or the algorithm is failing to pick up on them. In the <em>u-g</em>/<em>g-r</em> plot on the left hand side their is also a clear vertical boundary on the red stellar classification. This lines up exactly with where the stars with lowest <em>u-g</em> line ratio lie in the training set, suggesting that our model isn’t able to classify stars beyind this range.</p> <p>Given that we don’t know what is actually in the ‘test’ data set, it’s hard to draw any firm conclusions from it, but it does highlight some of the limitations of such learning algorithms, namely that they are very bad at predicting events beyond what they’ve been trained to; this is more generally known as ‘overfitting’, in relation to the training set.</p> <p>All of the code used to produce this post is available <a href="https://github.com/christopherlovell/statistical_learning/blob/master/SDSS_decision_trees.Rmd" rel="external nofollow noopener" target="_blank">here</a>. Thanks for reading.</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2019/sphinx-documentation/">An introduction to creating Python documentation using Sphinx</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/mondrian-generator/">D3 Mondrian Generator</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2017/ssh-jupyter/">Accessing Jupyter over SSH</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/spotmybackup/">Back up your Spotify Playlists</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2016/convert-wav-mp3/">Converting WAV to MP3 in the terminal</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Chris Lovell. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"in reverse chronological order.",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-projects",title:"projects",description:"A growing collection of your cool projects.",section:"Navigation",handler:()=>{window.location.href="/projects/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in '_pages/cv.md'. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-an-introduction-to-creating-python-documentation-using-sphinx",title:"An introduction to creating Python documentation using Sphinx",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/sphinx-documentation/"}},{id:"post-d3-mondrian-generator",title:"D3 Mondrian Generator",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/mondrian-generator/"}},{id:"post-accessing-jupyter-over-ssh",title:"Accessing Jupyter over SSH",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/ssh-jupyter/"}},{id:"post-back-up-your-spotify-playlists",title:"Back up your Spotify Playlists",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/spotmybackup/"}},{id:"post-converting-wav-to-mp3-in-the-terminal",title:"Converting WAV to MP3 in the terminal",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/convert-wav-mp3/"}},{id:"post-bayes-39-rule-james-v-stone",title:"Bayes' Rule, James V. Stone",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/bayes-rule-book/"}},{id:"post-astro-in-python",title:"Astro in Python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/astro-in-python/"}},{id:"post-highly-original-monster-catching-game",title:"Highly Original Monster Catching Game",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/bokemon/"}},{id:"post-taking-py-sphviewer-for-a-spin",title:"Taking Py-SPHViewer for a spin",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/py-sphviewer/"}},{id:"post-nick-bostrom-astronomical-waste",title:"Nick Bostrom: Astronomical Waste",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/opportunity-cost-delayed-colonisation/"}},{id:"post-image-classification-with-principal-component-analysis",title:"Image Classification with Principal Component Analysis",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/image-pca-deckchair/"}},{id:"post-log-normal-fitting-and-q-q-plots-in-r",title:"Log-normal fitting and Q-Q plots in R",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/lognormal-fit-QQ-R/"}},{id:"post-survey-bias",title:"Survey Bias",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/survey-bias/"}},{id:"post-h5py-reading-and-writing-hdf5-files-in-python",title:"h5py: reading and writing HDF5 files in Python",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/h5py-intro/"}},{id:"post-deriving-the-jean-39-s-mass",title:"Deriving the Jean's Mass",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/jeans-mass/"}},{id:"post-cosmography-of-the-local-universe",title:"Cosmography of the local universe",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/cosmography-of-the-local-universe/"}},{id:"post-maxwell-39-s-equations-in-terms-of-potentials",title:"Maxwell's equations in terms of potentials",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/maxwells-equations-potentials/"}},{id:"post-the-kardashev-scale-an-introduction",title:"The Kardashev Scale: An Introduction",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/kardashevs-scale/"}},{id:"post-my-first-astrobite",title:"My First Astrobite",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/first-astrobite/"}},{id:"post-meeting-the-f-amp-w-in-nfw",title:"Meeting the F & W in NFW",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/nfw-profile/"}},{id:"post-lord-kelvin-and-the-age-of-the-earth",title:"Lord Kelvin and the age of the earth",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/The-age-of-the-earth/"}},{id:"post-gchq-christmas-puzzle",title:"GCHQ Christmas Puzzle",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/GCHQ-christmas-puzzle/"}},{id:"post-orwell-39-s-support-for-the-rising",title:"Orwell's support for the Rising",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/orwell-warsaw/"}},{id:"post-classifying-astronomical-data-using-tree-based-methods",title:"Classifying Astronomical Data Using Tree Based Methods",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/sdss-decision-trees/"}},{id:"post-what-can-we-learn-from-the-history-of-physics-publications",title:"What can we learn from the history of physics publications?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/nature-physics-publications/"}},{id:"post-topic-modelling-no-10-39-s-speeches",title:"Topic modelling No.10's speeches",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/topic-modelling-10-speeches/"}},{id:"post-educating-undergraduates-a-guide-by-roger-blin-stoyle",title:"Educating undergraduates, a guide by Roger Blin-Stoyle",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/z-associate-tutoring/"}},{id:"post-download-d3-vis-using-svg-crowbar",title:"Download D3 vis using SVG crowbar",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/svg-crowbar/"}},{id:"post-how-to-build-a-galaxy-1",title:"How to build a galaxy, #1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/third-year-project/"}},{id:"post-tweets-runs-and-the-minnesota-vikings",title:"Tweets, Runs and the Minnesota Vikings",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/bank-underground/"}},{id:"post-creating-dynamic-ui-components-in-shiny",title:"Creating dynamic UI components in Shiny",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/dynamic-ui-shiny/"}},{id:"post-phd-at-uos",title:"PhD at UoS",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/reveal-phd/"}},{id:"post-using-monte-carlo-methods-to-estimate-pi",title:"Using Monte Carlo methods to estimate pi",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/monte-carlo-pi/"}},{id:"post-a-failed-twitter-election-analysis-and-lessons-learnt",title:"A failed twitter election analysis, and lessons learnt",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/failed-twitter-election-analysis/"}},{id:"post-uk-domestic-airport-traffic-a-shiny-investigation",title:"UK Domestic Airport Traffic: A Shiny Investigation",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/d3network-shiny-app/"}},{id:"post-association-rule-mining-in-r",title:"Association Rule Mining in R",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/association-rule-mining-R/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%63%68%72%69%73%74%6F%70%68%65%72.%6C%6F%76%65%6C%6C@%70%6F%72%74.%61%63.%75%6B","_blank")}},{id:"socials-orcid",title:"ORCID",section:"Socials",handler:()=>{window.open("https://orcid.org/0000-0001-7964-5933","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=2wlPQ1QAAAAJ","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/christopherlovell","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/lovellchristopher","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>